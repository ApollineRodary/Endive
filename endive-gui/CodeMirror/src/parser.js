// This file was generated by lezer-generator. You probably shouldn't edit it.
import {LRParser} from "@lezer/lr"
const spec_Identifier = {__proto__:null,Lemma:46, Exercise:48, Qed:50, forall:54, exists:56, pouet:58, Prop:60, Set:62, exact:66, intro:68, let:70}
export const parser = LRParser.deserialize({
  version: 14,
  states: "&xQYQPOOOOQO'#Ca'#CaOeQPO'#C`O!OQPO'#C_OOQO'#Cl'#ClQYQPOOO!VQPO,58zO!hQPO,58zO!mQQO,58yOOQO'#Cj'#CjO!xQQO'#CiOOQO'#Cm'#CmO#QQPO,58yOOQO-E6j-E6jOOQO'#Ce'#CeO#XQQO'#CdOOQO'#Cd'#CdO#dQQO'#CcO#lQQO1G.fO!VQPO1G.fOOQO'#Ck'#CkOOQO1G.e1G.eO#qQQO'#CnO#|QQO,59TOOQO,59T,59TO!mQQO1G.eOOQO-E6k-E6kO$UQPO,58}O!VQPO,58}OOQO7+$Q7+$QO$^QQO7+$QO$cQPO,59YOOQO-E6l-E6lOOQO1G.o1G.oOOQO7+$P7+$PO!VQPO1G.iO$nQPO1G.iOOQO1G.i1G.iOOQO<<Gl<<GlOOQO'#Ch'#ChOOQO1G.t1G.tOOQO7+$T7+$TO$yQPO7+$TO!VQPO<<GoOOQOAN=ZAN=Z",
  stateData: "%O~OeOSPOS~OgPOhPOiPO~OUVOjUO~OgPOhPOqXOrXOsXO~OiWO~PmOU`OY`Ok^Ol^Om^O~OjcO~OfdOUTXjTX~OUfOfhO~OiiO~PmOUkOZWXfWX~OZlOfVX~OfmO~OjoOUbXfbX~OUfOfqO~OjtOpsO~OfvO~OUxOnwOowO~OUzOnwOowO~Op{O~O",
  goto: "#ZcPPPdlrPx!X!`PP!gh!m!q!w!}#TSSOTTZR[XRORT[XQORT[QbUQncQulQysR|{ZaUcls{Z_Ucls{QxoRztTYR[QeWRriQTOR]TQ[RRj[QgYRpg",
  nodeNames: "âš  LineComment Source Block Declaration Lemma Identifier Expression Value Keyword Number Binop Type Action Tactic Final",
  maxTerm: 35,
  skippedNodes: [0,1],
  repeatNodeCount: 3,
  tokenData: "$X~R[XYwYZw]^wpqw|}!Y}!O!_!O!P!j!P!Q!z!Q![#c![!]#t!c!}#y#T#o#y~|Se~XYwYZw]^wpqw~!_Op~~!bP!`!a!e~!jOZ~R!oPfQ!Q![!rP!wPYP!Q![!r~#PSP~OY!zZ;'S!z;'S;=`#]<%lO!z~#`P;=`<%l!zP#hQYP!O!P#n!Q![#cP#qP!Q![!r~#yOj~~$ORU~!Q![#y!c!}#y#T#o#y",
  tokenizers: [0, 1],
  topRules: {"Source":[0,2]},
  specialized: [{term: 6, get: (value) => spec_Identifier[value] || -1}],
  tokenPrec: 0
})
